\documentclass[catalog.tex]{subfiles}

% do not write anything in the preamble

\begin{document}

\def\pbname{Naive Bayes Classifier} %change this, do not use any number, just the name

\section{\pbname} 

% only for overview, so short description (no more than 1-2 lines)
\begin{overview}
\item [Algorithm:] Naive Bayes Classifier(algo.~\ref{alg:\currfilebase}) 
	% -	must match the label of the algorithm 
	% - when writing more than one algo use alg:\currfilebase_a, alg:\currfilebase_b, etc.
\item [Input:] The training set $T = \{(x_1,y_1), (x_2, y_2), \cdots, (x_n,y_n)\}$; $x_j^{(i)}$ is the $j^{th}$ feature of the $i^{th}$ sample;$a_{jl}$ is the $l$ possible values of the $j^{th}$ feature
\item [Complexity:] $\mathcal{O}(nk)$
\item [Data structure compatibility:] N/A 
\item [Common applications:] Artificial intelligence
\end{overview}


\begin{problem}{\pbname}
	Naive Bayesian Classification is a classification method based on Bayesian Theorem and Conditional Independence Assumption.
\end{problem}


\subsection*{Description}
\subsubsection{Bayesian Theorem}
Bayes's theorem is stated as\cite{10.5555/59556}
\begin{equation}
P(A|B) = \frac{P(B|A)P(A)}{P(B)}
\end{equation}
where A and B are events and $P(B)\neq0$.
\subsubsection{Naive Bayes Classifier}
Assume input space $X\in \mathbb{R}^n$ is a $n$ dimension vector and the label set of input $Y=\{c_1, c_2, \cdots, c_K\}$. The training set can be denoted as 
\begin{equation}
T = \{(x_1,y_1), (x_2, y_2), \cdots, (x_n,y_n)\}
\end{equation}
which is generated by $P(X,Y)$ independently.
\par The goal of Naive Bayes is to learn a joint distribution $P(X,Y)$. Specially, it should learn prior distribution and conditional distribution. The prior distribution is
\begin{equation}
P(Y=c_k),\quad k=1,2,\cdots,K
\end{equation}
The conditional distribution is 
\begin{equation}
P(X=x|Y=c_k) = P(X_1 = x_1, \cdots X_n = x_n|Y=c_k)
\end{equation}
Naive Bayes makes a conditional independence assumption, which is
\begin{equation}
P(X=x|Y=c_k) = \prod_{j=1}^{n}P(X_=x_j|Y=c_k)
\label{eqn:\currfilebase_a}
\end{equation}
Naive Bayes will use input $x$ and output the class by the learned largest posterior distribution $P(Y=c_k|X=x)$. The posterior can be calculated by using Bayesian Theorem and equation.~\ref{eqn:\currfilebase_a}
\begin{align}
P(Y=c_k|X=x) &= \frac{P(X=x|Y=c_k)P(Y=c_k)}{\sum_kP(X=x|Y=c_k)P(Y=c_k)}\\
&= \frac{P(Y=c_k)\prod_{j=1}^{n}P(X_=x_j|Y=c_k)}{\sum_kP(Y=c_k) \prod_{j=1}^{n}P(X_=x_j|Y=c_k)}, \quad k=1,2,\cdots, K
\label{eqn:\currfilebase_b}
\end{align}
Then, the Naive Bayes can be represented as
\begin{align}
y &=\arg \max_{c_k}\frac{P(Y=c_k)\prod_{j=1}^{n}P(X_=x_j|Y=c_k)}{\sum_kP(Y=c_k) \prod_{j=1}^{n}P(X_=x_j|Y=c_k)}\\
&=\arg \max_{c_k} P(Y=c_k)\prod_{j=1}^{n}P(X_=x_j|Y=c_k)
\label{eqn:\currfilebase_c}
\end{align}
\subsubsection{Maximum Likelihood Estimation(MLE)}
We can use MLE to estimate prior probability $P(Y=c_k)$.
\begin{equation}
P(Y=c_k) = \frac{\sum_{i=1}^{N}I(y_i=c_k)}{N}, \quad k=1,2,\cdots,K
\label{eqn:\currfilebase_d}
\end{equation}
Assume possible value set of the $j^{th}$ feature $x_j$ is $\{a_{j1}, a_{j2},\cdots, a_{jS_j}\}$. The estimation of conditional probability $P(X_j=a_{jl}|Y=c_k)$ is
\begin{align}
P(X_j=a_{jl}|Y=c_k) = \frac{\sum_{i=1}^{N}I(x_j^{(i)}=a_{jl}, y_i=c_k)}{\sum_{i=1}^NI(y_i=c_k)}\\
j=1,2,\cdots,n;\quad l=1,2,\cdots,S_j;\quad k=1,2,\cdots,K
\label{eqn:\currfilebase_e}
\end{align}
where $x_j^{(i)}$ is the $j^{th}$ feature of the $i^{th}$ sample; $a_{jl}$ is the $l$ possible values of the $j^{th}$ feature.
\begin{Algorithm}[Naive Bayes Classifier\label{alg:\currfilebase}]
	% -	must match the reference in the overview
	% - when writing more than one algo use alg:\currfilebase_a, alg:\currfilebase_b, etc.
	%\SetKwFunction{myfunction}{MyFunction}	
	\Input{The training set $T = \{(x_1,y_1), (x_2, y_2), \cdots, (x_n,y_n)\}$; $x_j^{(i)}$ is the $j^{th}$ feature of the $i^{th}$ sample;$a_{jl}$ is the $l$ possible values of the $j^{th}$ feature}
	\Output{class of the sample $x$: $y$}
	%	\Fn{\myfunction{$a,b$}}{
	%	}
	Calculate prior distribution by equation.~\ref{eqn:\currfilebase_d} and conditional distribution by equation.~\ref{eqn:\currfilebase_e}.\\
	Calculate posterior distribution with sample $x=(x_1, x_2,\cdots, x_n)^{\mathrm{T}}$ and equation.~\ref{eqn:\currfilebase_b}.\\
	Find the class $y$ of $x$ with equation.~\ref{eqn:\currfilebase_c}.
	\BlankLine

	\Ret $y$

\end{Algorithm}


% include references where to find information on the given problem using latex bibliography
% insert references in the text (\cite{}) and write bibliography file in problem-nb.bib (replace nb with the problem number)
% prefer books, research articles, or internet sources that are likely to remain available over time
% as much as possible offer several options, including at least one which provide a detailed study of the problem
% if available include links to programs/code solving the problem
% wikipedia is NOT acceptable as a unique reference
\singlespacing
\printbibliography[title={References.},resetnumbers=true,heading=subbibliography]

\end{document}
